---
title: "Data Mining HW3"
author: "Arman Zakaryan"
date: "October 21, 2017"
output: html_document
---

##Problem 1

```{r}
load('Wholesale.rda')
rownames(df) <- df$customer_id
df$customer_id <- NULL

```

##Problem 2

```{r}
class(df$Channel)
class(df$Region)
df$Channel <- factor(df$Channel, levels=c(1,2), labels=c('HoReCa', 'Retail'))
df$Region <- factor(df$Region, levels=c(1,2,3), labels=c('Lisbon', 'Oporto', 'Other Region'))

```

##Problem 3

```{r}
dsmall <- head(df,n=10)
dsmall_dist <- dist(dsmall, method = 'euclidian')
dsmall_dist
```

##Problem 4

When using complete linkage method for hierarchial clustering, the distance between two clusters is considered the distance between the furthest members of each cluster (i.e. in case we have 2 clusters $A$ and $B$ and members $a_1...a_n$ and $b_1...b_m$, then $dist(A,B) = max(dist(a_i,b_j)  0\leq i \leq n, 0\leq j \leq m$)).
```{r}
set.seed(1)
hc <- hclust(dsmall_dist, method = 'complete')
```

##Problem 5

```{r}
plot(hc)
rect.hclust(hc, k = 3)
```
Distance of Customer 10 from all other existing outlined clusters is greater, than the ones in-between the outlined clusters, so it got left out.

##Problem 6

```{r}
hc$merge
```
Basically shows at each step which cluster was merged with which. Negative sign means singleton entry, positive means a cluster. First row shows that Customer 1 and Customer 6 were merged into a cluster, second row shows that Customer 8 and 9 have been merged into another cluster, and the third row shows that Customer 2 was merged with the cluster formed in row 1.

##Problem 7

Normalizes the data. That is, subtracts the mean and divides by the standard deviation of the corresponding columns.
```{r}
dsmall[,3:8] <- scale(dsmall[,3:8], center=T, scale=T)
```

##Problem 8

```{r}
set.seed(1)
hc1 <- hclust(dist(dsmall), method = 'complete')
plot(hc1)
rect.hclust(hc1, k = 3)
```
Changed the result significantly, as almost all the clusters have been rearranged. The reason is, that the distance between the Customers has changed. Normalizing the data bring all components to the same scale, so the distances may indeed vary, hence the new dendrogram.

##Problem 9

```{r}
wdi <- read.csv('WDI indicators.csv')
wdi <- na.omit(wdi)
wdi_scaled <- as.data.frame(scale(wdi[,3:10], center=T, scale=T))
```

##Problem 10

Using the "Elbow Method"
```{r}
tots={}
for(i in c(1:10)){
  set.seed(1)
  tots[i]=kmeans(wdi_scaled, i)$tot.withinss
}
plot(1:10,tots, type='b', pch=19, frame=FALSE)
opt_clust <- 5
```
The optimal number appears to be 5.

##Problem 11

```{r}
set.seed(1)
km <- kmeans(wdi_scaled, opt_clust)
wdi$cluster <- as.factor(km$cluster)
```

##Problem 12

```{r}
aggregate(.~cluster, data=wdi, FUN = 'mean')
```
Clusters 1 and 5 appear to be very developed countries judging by GDP and internet access. Cluster 3 is obviously underdeveloped lower-end third world countries. And clusters 2 and 4 are more on the higher end of the third world countries, cluster 4 falling a little behind.

##Problem 13

An inverse relationship between GDP and Population seems to prevail in all clusters. Which enforces the above mentioned assumptions.

##Problem 14

```{r}
agg <- aggregate(Country~cluster, data=wdi, print)
```
Armenia is in the cluster of higher end 3rd world countries, so assumptions were true.

##Problem 15

```{r}
library(FactoMineR)
ca <- PCA(wdi[3:10], ncp=8, graph = F)
plot(ca,choix="var", cex=0.5,invisible="quanti.sup")
summary(ca)
```
Dim1 explains 38.87% of the variance in data, Dim2 explains 19.72% of total variance.
Dim1 correlates strongly with NE.IMP.GNFS.ZS and NE.EXP.GNFS.ZS
Dim2 correlates strongly with NY.GDP.PCAP.CD, IT.NET.USER.P2, SP.DYN.LE00.
And the above mentioned are the most corresponding variables.
And looking at eigenvalues, we see that the first 4 Principal Components explain 83.5% of total variance.
